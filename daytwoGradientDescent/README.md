# Day 2 â€” Gradient Descent from Scratch
## MLIDOMM: Machine Learning Is the Daughter of Mathematics

This project represents **Day 2** of the MLIDOMM challenge, shifting from closed-form solutions to **calculus-driven learning** using **Gradient Descent**.

The focus is on understanding **how machines learn by minimizing loss functions** through differential calculus.

---

## ğŸ“Œ Objective

- Derive Gradient Descent mathematically
- Implement it manually using Python
- Visualize the learning process
- Connect calculus theorems to ML behavior

---
## ğŸ“ Mathematical Concepts Covered

- Partial derivatives
- Gradients
- Convex functions
- Taylor series approximation
- Optimization and convergence

---

## ğŸ“Š Visualizations

- **Loss vs Iterations**
- Demonstrates monotonic decrease of cost
- Confirms convergence due to convexity

---

## ğŸ› ï¸ Implementation Details

- Language: **Python**
- Libraries: **NumPy**, **Matplotlib**
- No use of `scikit-learn`
- Fully vectorized gradient computation

---

## ğŸ“‚ Project Structure

.
â”œâ”€â”€ gradient_descent_manual.py
â”œâ”€â”€ README.md

---

## ğŸ“ˆ Learning Outcome

- Manual implementation of Gradient Descent
- Verified convergence behavior
- Clear interpretation of learning rate and optimization dynamics

---

## ğŸ¯ PG Entrance Mapping

- Differential Calculus
  - Partial derivatives
  - Gradient vectors
- Optimization
  - Minima and convergence
- Convexity
  - Global minimum guarantee

---

## ğŸ”® Next Steps

Day 3 will extend this foundation to:
- Hessian matrices
- Eigenvalues and conditioning
- Regularization (Ridge & Lasso)
- Biasâ€“Variance tradeoff

---

**Author:** Tanmay Kumar Chaki
**Challenge:** MLIDOMM  

