<<<<<<< HEAD
# Day 3 â€” Regularization in Linear Regression  
### MLIDOMM: *Machine Learning Is the Daughter of Mathematics*

---

## ğŸ“Œ Objective

Day 3 focuses on **Regularization**, a fundamental concept that connects **linear algebra, calculus, optimization, and statistical learning theory**.

This mini project demonstrates, both **mathematically and empirically**:
- Why Ordinary Least Squares (OLS) tends to overfit
- How **Ridge (L2)** and **Lasso (L1)** regularization improve generalization
- The effect of regularization on numerical stability and model complexity

---

## ğŸ§  Mathematical Foundation

### 1. Ordinary Least Squares (OLS)

The Mean Squared Error loss is defined as:


**Limitation**
- When features are highly correlated, \( X^T X \) becomes ill-conditioned
- Leads to unstable coefficients and poor generalization

---

### 2. Ridge Regression (L2 Regularization)

Adds a penalty on the squared magnitude of coefficients

**Effect**
- Improves numerical conditioning
- Reduces variance
- Introduces small bias for better generalization

---

### 3. Lasso Regression (L1 Regularization)


**Key Properties**
- No closed-form solution
- Produces sparse coefficients
- Performs implicit feature selection

---

## ğŸ›  Mini Project Description

A synthetic dataset with **high multicollinearity** is created to expose the weaknesses of OLS and justify regularization.

### Implementations:
- Ordinary Least Squares (manual)
- Ridge Regression (manual closed-form)
- Lasso Regression (solver-based)
- Train/Test evaluation
- Coefficient comparison visualization

---

## ğŸ“‚ Project Structure
Day-3-Regularization/
â”‚
â”œâ”€â”€ regularization_demo.py
â”œâ”€â”€ README.md


---

## ğŸ“Š Results Summary

| Model | Training Error | Test Error | Coefficient Stability |
|------|---------------|------------|-----------------------|
| OLS  | Very Low | High | Unstable |
| Ridge | Moderate | Lower | Stable |
| Lasso | Moderate | Competitive | Sparse |

---

## ğŸ“ˆ Visualization

The project visualizes:
- Coefficient shrinkage across OLS, Ridge, and Lasso
- The stabilizing effect of regularization on learned parameters

---

## ğŸ“ PG Entrance Mapping

- Differential Calculus â†’ Optimization
- Linear Algebra â†’ Conditioning and eigenvalues
- Convexity â†’ Guaranteed global minima
- Biasâ€“Variance Tradeoff â†’ Statistical learning theory

---

## ğŸš€ Key Takeaway

> Regularization is not a heuristic â€” it is a mathematical necessity  
> for stable and generalizable machine learning models.

---

## ğŸ”– Tags

Machine Learning Â· Mathematics Â· Regularization Â· Ridge Regression Â· Lasso Â· Optimization Â· From Scratch Â· MLIDOMM
=======
# Day 3 â€” Regularization in Linear Regression  
### MLIDOMM: *Machine Learning Is the Daughter of Mathematics*

---

## ğŸ“Œ Objective

Day 3 focuses on **Regularization**, a fundamental concept that connects **linear algebra, calculus, optimization, and statistical learning theory**.

This mini project demonstrates, both **mathematically and empirically**:
- Why Ordinary Least Squares (OLS) tends to overfit
- How **Ridge (L2)** and **Lasso (L1)** regularization improve generalization
- The effect of regularization on numerical stability and model complexity

---

## ğŸ§  Mathematical Foundation

### 1. Ordinary Least Squares (OLS)

The Mean Squared Error loss is defined as:


**Limitation**
- When features are highly correlated, \( X^T X \) becomes ill-conditioned
- Leads to unstable coefficients and poor generalization

---

### 2. Ridge Regression (L2 Regularization)

Adds a penalty on the squared magnitude of coefficients

**Effect**
- Improves numerical conditioning
- Reduces variance
- Introduces small bias for better generalization

---

### 3. Lasso Regression (L1 Regularization)


**Key Properties**
- No closed-form solution
- Produces sparse coefficients
- Performs implicit feature selection

---

## ğŸ›  Mini Project Description

A synthetic dataset with **high multicollinearity** is created to expose the weaknesses of OLS and justify regularization.

### Implementations:
- Ordinary Least Squares (manual)
- Ridge Regression (manual closed-form)
- Lasso Regression (solver-based)
- Train/Test evaluation
- Coefficient comparison visualization

---

## ğŸ“‚ Project Structure
Day-3-Regularization/
â”‚
â”œâ”€â”€ regularization_demo.py
â”œâ”€â”€ README.md


---

## ğŸ“Š Results Summary

| Model | Training Error | Test Error | Coefficient Stability |
|------|---------------|------------|-----------------------|
| OLS  | Very Low | High | Unstable |
| Ridge | Moderate | Lower | Stable |
| Lasso | Moderate | Competitive | Sparse |

---

## ğŸ“ˆ Visualization

The project visualizes:
- Coefficient shrinkage across OLS, Ridge, and Lasso
- The stabilizing effect of regularization on learned parameters


---

## ğŸš€ Key Takeaway

> Regularization is not a heuristic â€” it is a mathematical necessity  
> for stable and generalizable machine learning models.

---

## ğŸ”– Tags

Machine Learning Â· Mathematics Â· Regularization Â· Ridge Regression Â· Lasso Â· Optimization Â· From Scratch Â· MLIDOMM

>>>>>>> 565ac4466e9c3e6d97488af59fe8d8f8b2fe907e
