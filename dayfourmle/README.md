# Day 4 â€” Maximum Likelihood Estimation for Linear Regression  
### MLIDOMM: Machine Learning Is the Daughter of Mathematics

---

## ðŸ“Œ Objective

Day 4 establishes the **probabilistic foundation of Linear Regression** by showing that the commonly used Mean Squared Error loss arises naturally from **Maximum Likelihood Estimation (MLE)** under Gaussian noise assumptions.

This project demonstrates, through mathematics and code, that:

> Minimizing Mean Squared Error is equivalent to maximizing the likelihood of the data.

---

## ðŸ›  Mini Project Description

This project implements:

- Synthetic data generation with Gaussian noise
- Closed-form Maximum Likelihood Estimation
- Mean Squared Error computation
- Log likelihood computation
- Numerical verification of MSEâ€“MLE equivalence
- Visualization of loss and likelihood curves

All implementations are done **from scratch using NumPy**, without ML libraries.

---

## ðŸ“‚ Project Structure

Day-4-MLE-Linear-Regression/
â”‚
â”œâ”€â”€ mle_linear_regression.py
â”œâ”€â”€ README.md

yaml
Copy code

---

## ðŸ“Š Key Observations

- MSE reaches its minimum at the same parameter value where log likelihood reaches its maximum
- Linear Regression parameters are the most probable parameters under Gaussian noise
- The optimization view and probabilistic view are mathematically identical

---

## ðŸŽ“ Academic and PG Entrance Mapping

- Probability density functions
- Gaussian distribution
- Log likelihood
- Optimization via differentiation
- Linear algebra and matrix calculus

---

## ðŸš€ Key Takeaway

> Linear Regression minimizes Mean Squared Error because it is the Maximum Likelihood Estimator under Gaussian noise assumptions.

---

## ðŸ”– Tags

Machine Learning Â· Probability Â· Statistics Â· Maximum Likelihood Estimation Â· Linear Regression Â·
