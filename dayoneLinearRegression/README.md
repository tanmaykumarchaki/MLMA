# Day 1 â€” Linear Regression from Scratch
## MLIDOMM: Machine Learning Is the Daughter of Mathematics

This project marks **Day 1** of the MLIDOMM challenge, focusing on implementing **Linear Regression purely from mathematical principles**, without using machine learning libraries.

The goal is to bridge **pure mathematics** and **machine learning practice**, aligned with **PG entranceâ€“level concepts**.

---

## ğŸ“Œ Objective

- Understand Linear Regression as a **least squares optimization problem**
- Derive and implement the **Normal Equation**
- Work strictly with **matrix algebra**
- Build a mini capstone project using Python + NumPy

---


## ğŸ§  Key Concepts Covered

- Least Squares Method
- Matrix Transpose and Inverse
- Bias Term Augmentation
- Vectorized Computation
- Linear Algebra in ML

---

## ğŸ› ï¸ Implementation Details

- Language: **Python**
- Libraries: **NumPy**
- No use of `scikit-learn`
- All vectors treated as **2D matrices** to preserve mathematical correctness

---

## ğŸ“‚ Project Structure
.
â”œâ”€â”€ linear_regression_manual.py
â”œâ”€â”€ README.md


---

## ğŸ“ˆ Results

- Learned model parameters (bias & weight)
- Verified predictions using matrix multiplication
- Compared actual vs predicted outputs

---

## ğŸ¯ PG Entrance Mapping

- Linear Algebra
  - Matrix multiplication
  - Transpose
  - Inverse
- Optimization
  - Least squares minimization
- Vector spaces and projections

---

## ğŸ”— Next Step

Day 2 extends this work by introducing **calculus-based optimization using Gradient Descent**, explaining *how machines learn*.

---

**Author:** Tanmay Kumar Chaki
**Challenge:** MLIDOMM  

